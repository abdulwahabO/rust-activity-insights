

	
>>>>> Data source is Github's Events API:
		--> Find events from an organisation -- e.g rustlang. That way I get events from all their repositories.
		--> Write new-line delimited JSON if supported by Jackson/GSON otherwise use Parquet.

>>>>> Final Data Model: See file in this directory for 
		
		  > Each item in the DynamoDB table has a date field which is the primary key. And repositories data is embedded.
		  > SEE FILE IN THIS DIRECTORY.

	Load JSON data from S3 into strongly typed Dataset<T> Spark using as(...) method and supplying an Encoder.

	After all transformations on datasets, collect the final result back to the driver program and write it to DynamoDB directly.

	NOTE: If AWS Java SDK 2.0 cannot be used to effectively query by Date, then load all items and do it manually, There are only 
			365 days in a year after all.


>>>>> Frontend: Charts display data for a specific period. The period can be changed using a date range picker.

		--> A doughnut pie chart showing ---> The percentages of activities from the top five repositories for that period.

		--> A grouped bar chart showing ---> 

		--> A stripped table with columns ----> Details of each repository for that period.
				--> Github users on the table should be behind a link that shows a card/modal with their details.

  		--> A date range picker (Use date range picker library)
					----> THIS IS IMPORTANT SO THE PROJECT IS INTERACTIVE, OTHERWISE THE GIF WOULD LOOK DRAB.

		--> Use Monospace font.

			--> For example of what to build see the screenshot on this page: https://github.com/features/insights

	>> UI Layout: 
		- A header with name of the app in the middle.
		- A 12-width column with a date range picker on the left.
		- A 12-width column holding a text paragraph that explains the charts below it.
		- Two 6-width columns holding pie and bar charts.
		- A text paragraph that explains the table so that it's column's names need not be overly descriptive. 
		- A 12-width column below them holding the table.


>>> Business Case:

		(Todo: write this)



>>> Portfolio site should describe project as phases in pipeline

		> Ingestion: AWS Lambda
		> Distributed File System: S3 (S3 is actually an object store.. soooo?)
		> Processing - Apache Spark.
		> Data Store: PostgresSQL
		> Retrieval and visualization - Spring Boot + ChartJS running on AWS Elastic Beanstalk.


>>> Things to figure out how to do correctly:
		
		> Read and write data from and to S3 using Spark's APIs.
		> Obtain a URL for an object in S3.
		> Make Spark download the most recent file.


>> Pipeline Overview:
		
		Github Events API -> Lambda -> S3 -> Spark -> S3 -> DynamoDB -> Beanstalk Web Application.
				
	The web application that serves the frontend will also call Github's API to retrieve details of any new users that are not saved.

>> Project Repository layout:
		
		-- Lambda function code packaged as a Shaded JAR.
		-- Spark Application packaged as shaded JAR.
		-- Spring Boot web application. 



>>> Names of projects and components:

		The name of project: Rust Contribution Insights           -- (can't be Github Insights because that exists)
		
		extractor:
		    ---> JAR function that pulls from Github.
		    ---> Deletes old file on S3.
		    ---> Writes new file to S3.
		    ---> Tells the Spark Cluster Manager to start the Spark job.
		    ---> Save ETag from Github to S3 file or any kind of disk storage AWS Lambda allows.

		processor ---> The Spark Program.
		
							
		

	






See https://github.com/agaiduk/AirAware/blob/master/README.md
    for example of how to describe my project.















