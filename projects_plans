

	
>>>>> Data source is Github's Events API:
		--> Find events from an organisation -- e.g rustlang. That way I get events from all their repositories.


>>>>> Final Data Model: See file in this directory for 
		
		  > Each item in the DynamoDB table has a date field which is the primary key. And repositories data is embedded.
		  > SEE FILE IN THIS DIRECTORY.

	NOTE: If AWS Java SDK 2.0 cannot be used to effectively query by Date, then load all items and do it manually, There are only 
			365 days in a year after all.

>>>>> Frontend: Charts display data for a specific period. The period can be changed using a date range picker.

		--> A doughnut pie chart showing ---> The percentages of total activity contributed by the five type of
		activities

		--> A SIMPLE bar chart showing ---> The top five repositories in terms of pushes.. use Comparator o


  		--> A date range picker (Use date range picker library)
					----> THIS IS IMPORTANT SO THE PROJECT IS INTERACTIVE, OTHERWISE THE GIF WOULD LOOK DRAB.

		--> Use Monospace font.

			--> For example of what to build see the screenshot on this page: https://github.com/features/insights

	>> UI Layout: 
		- A header with name of the app in the middle.
		- A 12-width column with a date range picker on the left.
		- A 12-width column holding a text paragraph that explains the charts below it.
		- Two 6-width columns holding pie and bar charts.
		- A text paragraph that explains the table so that it's column's names need not be overly descriptive. 
		- A 12-width column below them holding the table.

>>> Business Case:

		(Todo: write this) -- Not much to say, the real objective of this project is using Apache Spark and AWS.

        // todo -- the pipeline is not very robust and doesn't handle possible error cases.


>>> Portfolio site should describe project as phases in pipeline

		> Ingestion: AWS Lambda
		> Distributed File System: S3 (S3 is actually an object store.. soooo?)
		> Processing - Apache Spark.
		> Data Store: AWS DynamoDB.
		> Visualization - Spring Boot + ChartJS running on AWS Elastic Beanstalk.
		> Pipeline orchestration: AWS Step Functions or Apache Airflow (AWS Managed Airflow)

>>> Things to figure out how to do correctly:
		
		> Read and write data from and to S3 using Spark's APIs.
		> Obtain a URL for an object in S3.
		> Make Spark download the most recent file.

>> Pipeline Overview:
		
		Github Events API -> Lambda -> S3 -> Spark -> S3 -> DynamoDB -> Beanstalk Web Application.
				
	The web application that serves the frontend will also call Github's API to retrieve details of any new users that are not saved.

>> Project Repository layout:
		
		-- Lambda function code packaged as a Shaded JAR.
		-- Spark Application packaged as shaded JAR.
		-- Spring Boot web application. 

>>> Names of projects and components:

		The name of project: Rust Contribution Insights           -- (can't be Github Insights because that exists)
		
		extractor:
		    ---> JAR function that pulls from Github.
		    ---> Deletes old file on S3.
		    ---> Writes new file to S3.
		    ---> Tells the Spark Cluster Manager to start the Spark job.
		    ---> Save ETag from Github to S3 file or any kind of disk storage AWS Lambda allows.

		processor ---> The Spark Program.


See https://github.com/agaiduk/AirAware/blob/master/README.md
    for example of how to describe my project.

